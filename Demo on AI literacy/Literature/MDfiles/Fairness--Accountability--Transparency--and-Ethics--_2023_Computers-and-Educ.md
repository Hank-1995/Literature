# Fairness, Accountability, Transparency, and Ethics (FATE) in Artificial Intelligence (AI) and higher education: A systematic review

## Metadata
- **Author**: Bahar Memarian
- **Subject**: Computers and Education: Artificial Intelligence, 5 (2023) 100152. doi:10.1016/j.caeai.2023.100152
- **Creator**: Elsevier
- **Producer**: Acrobat Distiller 8.1.0 (Windows)
- **Creation Date**: D:20231220010202Z
- **Modification Date**: D:20231220023915Z
- **Source File**: Fairness--Accountability--Transparency--and-Ethics--_2023_Computers-and-Educ.pdf
- **Converted**: 2025-10-23 22:46:11

---

## Content

--- Page 1 ---

Computers and Education: Artificial Intelligence 5 (2023) 100152
Available online 26 June 2023
2666-920X/© 2023 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).
Fairness, Accountability, Transparency, and Ethics (FATE) in Artificial 
Intelligence (AI) and higher education: A systematic review 
Bahar Memarian *, Tenzin Doleck 
Simon Fraser University, Faculty of Education, Vancouver, British Columbia, Canada   
A R T I C L E  I N F O   
Keywords: 
Artificial intelligence 
Higher education 
Fairness 
Accountability 
Transparency 
Ethics 
A B S T R A C T   
Background: The use of Artificial Intelligence or AI is rising in higher education. With this rise, the morality of AI 
programs is being questioned. There is, as such, a need to understand how notions of Fairness, Accountability, 
Transparency, and Ethics or FATE are identified in the AI and higher education studies to date. 
Purpose: This systematic review paper aims to understand definitions and studies on FATE and AI in the higher 
education literature. The contribution of this work is to provide a summary of FATE development and the 
synthesis of the challenges and potentials of each of the reviewed studies. 
Method: A total of 33 publications from SCOPUS and Web of Science (Wo S) were included in this systematic 
literature review. We examined definitions of FATE noted in the reviewed articles (may have been multiple in 
each study) and grouped them into descriptive (understandable by laypeople) and technical (containing jargon) 
definitions. We also examined the main FATE term studied in detail in each reviewed article and grouped them 
into qualitative and quantitative studies. 
Results: Findings show more descriptive definitions exist (especially for fairness) and similarly quantitative 
definitions mostly emerge for Fairness. Findings also show more quantitative studies exist (especially for fairness) 
and qualitative definitions mostly emerge for ethics. Generally, though, there are more definitions than relevant 
studies conducted in the literature. 
Conclusion: This systematic literature review offers a summary of definitions and studies conducted for FATE 
terms and AI in the higher education literature. Future work may benefit from bridging the gap between 
laypeople and experts by linking descriptive definitions with technical ones as well as qualitative studies with 
quantitative ones. Moreover, future work can study accountability and transparency further and make the study 
of FATE terms more longitudinal, open-access, and reproducible.   
1. Introduction 
The application of Artificial Intelligence (herein referred to as AI) is 
surging in education (Bearman et al., 2022; Roll & Wylie, 2016). From 
student admissions and recruitment to adaptive learning and assess-
ment, AI is finding its place anywhere that may require even a touch of 
(induced) logic and automation. Here we regard AI as all the classifi-
cation, trainable, predictive, and smart models that can either follow a 
set logic or offer new logic along with the necessary data. 
Higher education programs and accreditation bodies traditionally 
required human educational practitioners to oversee processes and 
maintain quality assurance. With the host of AI algorithms and data 
analytics available, however, institutions, groups of power, and in-
dividuals more generally are persuaded to use AI. AI may thus be 
deployed at various levels in the higher education industry (e.g., a 
course to program level) (Becker et al., 2017). 
The literature surrounding AI in education has existed for more than 
three decades (Beck et al., 1996; Nwana, 1990; Woolf, 1991, 2015). Yet 
some of the challenges and potentials remain unsolved or undiscovered 
which may be in part due to the largely unchanged structure of educa-
tion (Woolf et al., 2013). Though the field of AI has flourished consid-
erably over time as suggested by the increased rate and areas of 
publications (Tang et al., 2021), still AI is a commodity in education, and 
its effect is explored primarily in a qualitative and general, rather than 
pragmatic and specific level (Garrett et al., 2020). 
With the growth of AI in education, its morality is being questioned 
(Holmes et al., 2021). Because of the rapid rate at which AI can mine 
datasets and predict or offer new models surrounding higher education 
* Corresponding author. 
E-mail address: bmemaria@sfu.ca (B. Memarian).  
Contents lists available at Science Direct 
Computers and Education: Artificial Intelligence 
journal homepage: www.sciencedirect.com/journal/computers-and-education-artificial-intelligence 
https://doi.org/10.1016/j.caeai.2023.100152 
Received 11 May 2023; Received in revised form 25 June 2023; Accepted 25 June 2023   

--- Page 2 ---

Computers and Education: Artificial Intelligence 5 (2023) 100152

programming, teaching, and learning, education may become a target of 
humans using poor logic or data and AI’s trial and error. 
From what it seems, AI can computationally and mathematically 
achieve the decision-making of humans and beyond (Holmes et al., 
2023, pp. 621–653). Yet in recent years concerns have emerged 
(Holmes, 2021; Jobin et al., 2019; Raji et al., 2021). And that is whether 
the many developments in the field of AI and its applications such as in 
education are in fact ethical and considerate of the needs and limitations 
of humans and the environment (Borenstein, J., & Howard, 2021; Sik-
dar, Lemmerich, & Strohmaier, 2022). 
Various objectives have emerged to characterize AI’s code of conduct 
at a more specific level. A term that has gained interest in AI education is 
“Fairness”, “Accountability”, Transparency” and “Ethics”—commonly 
referred to by the acronym “FATE” (Inuwa-Dutse, 2023; KPMG, 2019; 
Microsoft, 2023; Woolf, 2022). We should note that various other terms 
such as Explainable AI in education exist in the literature (Khosravi 
et al., 2022). Yet we find such terms to be holistic enough to require their 
systematic literature reviews. As such we wish to gain a better under-
standing of the FATE terms since they are most concerned about the 
morality footprint of AI in education. 
From a linguistic standpoint, each of the FATE terms has a broad and 
all-encompassing definition. A quick search of Merriam-Webster dic-
tionary, for example, suggests (Merriam-Webster, 2023): Fairness 
(noun) represents the quality or state of being fair. Fair (adjective) has 
several meanings, with the most relevant ones signifying impartiality 
and honesty, conforming to established rules, and being of average and 
acceptable quality. Accountability (noun) represents the quality or state 
of being accountable. Accountable (adjective) signifies being answer-
able and explainable. Transparency (noun) to represent the quality or 
state of being transparent. Transparent (adjective) has several meanings, 
with the most relevant ones signifying being readily understood, and 
having the accessibility of information. Ethics (noun) has several 
meanings, with the most relevant ones signifying a set of moral values, 
principles of conduct, and consciousness of moral values. 
We find the dictionary definitions of FATE to provide a big-picture 
overview of what each term denotes but lack a focused lens on AI and 
higher education. With more and more higher education institutions 
moving away from purely human to include artificially intelligent 
decision-makers, we find a review of FATE with AI and higher education 
to be timely and of interest for a broad readership. Understanding prior 
work can provide insight into the explored areas, challenges faced, and 
lessons learned in artificial morality. 
An exploration of systematic literature reviews on AI in education 
predominantly points to the efficient capabilities and personalized 
teaching and learning mechanisms promised by AI technology (Bozkurt, 
Karadeniz, Baneres, Guerrero-Roldan, & Rodriguez, 2021; Chen et al., 
2020; Woolf, 2015). What these review papers reveal are the many faces 
of AI in education and the need to understand the role of students, 
teachers, and administrators with AI appropriately. Application areas 
include but are not limited to the use of AI for assessment and evaluation 
in e-learning contexts (Tang et al., 2021), pedagogical support and 
profiling (Zawacki-Richter et al., 2019), and everyday institutional 
decision-making (Chen et al., 2020). As such we find a systematic review 
of FATE studies specifically to be lacking in the literature. 
A lack of proper understanding of how AI programs’ decisions impact 
important morality concerns may result in exasperating setbacks for 
different demographic groups or humankind more generally (Bearman 
et al., 2022). AI thinking tends to be hidden and looks like a black box to 
humans. As a result, efforts need to be made to characterize AI’s code of 
conduct. The lack of more specific (e.g., FATE) rather than general 
ethical concerns towards the use of AI in education along with no sys-
tematic literature review conducted on FATE terms in AI and education 
motivates our research. 
1.1. Purpose of the study 
Our review seeks to synthesize papers to present an overview of 
FATE in AI and higher education. The existing research helps understand 
the extent to which FATE has been studied. Therefore, we provide a 
systematic literature review of FATE in the context of AI and higher 
education. We seek to answer the following research questions by 
extracting data from the reviewed studies:  
1. RQ1: What definitions (one or a combination) of Fairness, 
Accountability, Transparency, and Ethics (FATE) have been dis-
cussed in studies covering AI and higher education?  
2. RQ2: How is FATE investigated in studies covering AI and higher 
education? 
This research contributes to the conceptualization and application of 
FATE in AI and higher education. In the methods section that follows, we 
share our search process, thematic coding, and analysis approach. The 
results section provides a demographic overview of the reviewed 
studies, followed by a summary of findings about our first and second 
research questions. The discussion section presents our synthesis of 
strengths, limitations, and future potentials of FATE in AI education. 
2. Materials and methods 
We follow a systematic search process and use the thematic coding 
described below to extract FATE-related data and chart the papers. 
2.1. Search process 
The Web of Science (WOS) and Scopus were used to search for ar-
ticles. We included articles published in international peer-reviewed 
journals and conference proceedings. Our search included publications 
written in English during an unlimited time frame. 
Our search for each database used the following search string: 
(“Fairness” OR “Accountability” OR “Transparency” OR “Ethics” OR 
“FATE”) AND (“AI” OR “Artificial Intelligence”) AND (“Higher educa-
tion” OR “University”) 
We sought to include all studies that had mention of one or multiple 
FATE terms, hence used OR operant. Since our focus is on AI and higher 
education settings, we include these terms via the AND operant. The 
overview of our search process is summarized in the PRISMA chart 
shown in Fig. 1. A total of 1292 (992 from WOS, 300 from SCOPUS with 
six duplicates removed) were included for screening. Our initial 
screening examined the title and abstract of 1286 studies. 
Our initial inclusion criteria contained review articles focusing on 
the role of AI or data science in higher education. Our initial exclusion 
criteria contained publications focused on the healthcare or non-higher 
education industries, leading to a total of 99 papers sought for retrieval. 
Of these, we were unable to retrieve 12 studies and conducted secondary 
screening for 87 studies. Our secondary screening examined the content 
of the manuscripts. Of the 87, 13 had the wrong population (e.g., K-12), 
11 had the wrong outcome (e.g., academic integrity), and six had the 
wrong study design (e.g., design ethics). This resulted in a total of 57 
studies included in the review. We further came to exclude 24 papers 
during the extraction process as we found them to be lacking informa-
tion regarding either of our research questions. 
Of the reviewed 33 studies, 17 (52%) were from WOS and 16 (48%) 
were from SCOPUS. The publications were 21 (64%) Journals and 12 
(36%) Conference articles. An overview of publication years of reviewed 
studies is presented in Fig. 2. Interestingly, we see an increased number 
of papers with mention of one or multiple FATE terms in 2022. We 
speculate this may be in part due to the open access release of AI pro-
grams like CHATGPT at the time, increasing attention, and thus studies 
surrounding FATE. A word cloud summary of predominant words used 
in the article tiles of the reviewed studies is presented in Fig. 3. As 
B. Memarian and T. Doleck                                                                                                                                                                                                                  

--- Page 3 ---

Computers and Education: Artificial Intelligence 5 (2023) 100152

expected, we find terms such as education, learn, AI, fairness, and 
general terms either related to the objectives of AI or characteristics of 
higher education appear the most in Fig. 3. 
2.2. Codification process 
Through our first research question, we share the definitions adopted 
or presented by the reviewed studies about FATE terms. We code and 
Records identified from*:
Databases (n =1292)
WOS (n = 992)
SCOPUS (n =300)
Registers (n = 0)
Records removed before 
screening:
Duplicate records removed 
(n = 6)
Records marked as ineligible 
by automation tools (n = 0)
Records removed for other 
reasons (n =0)
\
Records screened.
(n = 1286)
Records excluded**
(n = 1187)
Reports sought for retrieval.
(n = 99)
Reports not retrieved.
(n = 12)
Reports assessed for eligibility.
(n = 87)
Reports excluded:
Reason 1: Wrong population 
(n = 13)
Reason 2: Wrong outcome
(n = 11)
Reason 3: Wrong study 
design (n = 6)
Studies included in review.
(n = 57)
Reports of included studies
(n = 33)
Identification
Screening
Included
Fig. 1. PRISMA chart.  
Fig. 2. Publication year of reviewed studies.  
B. Memarian and T. Doleck                                                                                                                                                                                                                  

--- Page 4 ---

Computers and Education: Artificial Intelligence 5 (2023) 100152

summarize the definitions proposed for each of the FATE terms in the 
reviewed studies. In doing so, we come to find trends in key themes of 
FATE definitions presented in the literature. We further get a sense of 
studies that define in more descriptive vs. technical terms and in more 
vs. less detail. We searched for the terms “Fair”, “Accountab”, “Trans-
paren”, and “Ethic” in the PDF of each reviewed manuscript and 
extracted information for each term that signified a definition or 
consideration towards FATE terms. Much of the extracted information 
from the reviewed articles borrow ideas from other literature. Some 
build on the literature and presented new or modified definitions for 
FATE terms. We include both cited and proposed definitions in the 
reviewed studies. 
Through our second research question, we pinpoint the type of 
studies conducted surrounding FATE in the reviewed studies. We code 
and summarize the main FATE term study proposed in the reviewed 
studies. In doing so, we come to find trends in FATE studies, their re-
ported key challenges, and future potentials. We further get a sense of 
studies that explore in more qualitative vs. quantitative terms and in 
more vs. less detail. We extract such information predominantly from 
the abstract, methods, and results, as well as the conclusion sections. 
Our goal here is not to present a complete methodologic picture of each 
article, but to highlight the key feature or idea deployed in each study. 
Our protocol included the following steps:  
• Use the input string of search in both SCOPUS and Wo S.  
• Download the full records of all the papers.  
• Upload the full records to Covidence.  
• Use Covidence to identify duplicate studies and remove them.  
• Review the title and abstract of each article and decide if relevant or 
not (following initial inclusion and exclusion criteria as noted in 
section 2.1).  
• Download and review the full text of each article and decide if 
relevant or not (following secondary inclusion and exclusion criteria 
as noted in section 2.1).  
• Gather the full demographic records of studies selected for review.  
• Search for FATE terms as noted in 2.2.  
• Extract and chart definitions of FATE terms in each study in Excel. 
• Classify the definitions as being descriptive or technical. Any defi-
nitions that used jargon and advanced mathematical terms were 
technical.  
• Summarize each type of Qualitative and Technical definition for 
each of the FATE terms across the reviewed studies.  
• Extract what key FATE term each reviewed article studied. This was 
obtained by searching and finding the key FATE terms that were 
most referred to in each article and whether a qualitative or quan-
titative study was conducted surrounding that FATE term.  
• Classify through quantitative/quant (Likert style survey, empirical 
analysis) or qualitative/qual (literature review, reflective analysis, or 
open-ended interviews and focus groups) lens.  
• Summarize each type of Qualitative and Quantitative Study for each 
of the FATE terms across the reviewed studies.  
• Extract methods and findings of key FATE studies explored in each 
reviewed article.  
• Extract limitations and future work reported for the key FATE term 
explored in each reviewed article.  
• Create a synthesis of future recommendations and challenges in the 
discussion section based on the full review of each article. 
3. Results 
Our analysis of 33 reviewed articles aims to summarize definitions 
constructed and studies conducted surrounding FATE terms in the 
context of AI and higher education. We first explore content noted that 
relates to definitions of FATE terms in section 3.1. We then examine key 
themes of methods, findings, challenges, and future potentials noted 
surrounding FATE terms in section 3.2. 
Fig. 3. Predominant article title keywords.  
B. Memarian and T. Doleck                                                                                                                                                                                                                  

--- Page 5 ---

Computers and Education: Artificial Intelligence 5 (2023) 100152

3.1. Definitions of FATE noted in the reviewed articles 
A count summary of Fairness (F), Accountability (A), Transparency 
(T), and Ethics (E) terms from the reviewed studies and whether pre-
sented in Descriptive (D), or Technical (T) terms is presented in Fig. 4. 
Note that each reviewed article had a main focus (e.g., ethics) but may 
have included a description of multiple FATE terms (e.g., fairness and 
accountability) in descriptive or technical terms. Below we present a 
summary of key themes of the definitions for each of the FATE terms and 
further, break them down into descriptive and technical summaries. 
3.1.1. Fairness 
Fairness is the most noted FATE term in the reviewed studies (n=
24). Accordingly, the largest descriptive and technical representation of 
Fairness is provided in the reviewed studies. Note that studies may have 
explored how to achieve fairness or limit a lack of fairness, often noted 
as bias. Hence, below we summarize definitions presented surrounding 
Fairness achievement and bias mitigation that is presented in both 
descriptive and technical terms. 
3.1.1.1. Descriptive definitions of fairness or bias. In descriptive terms, 
Fairness in the context of AI and higher education is often denoted as 
either the landscape, culture, situation, or practice that attempted to 
make unfair practices just and/or mitigate bias. Higher education has 
many layers and stakeholders so as one can imagine, there is much room 
for improvement. A descriptive fairness definition is the most provided 
among the reviewed studies. 
A key emphasis is surrounding database and algorithmic bias to-
wards certain (often under-represented minority) groups. Shin et al. 
(2022) pose that fairness in AI contexts refers to algorithmic processes 
that do not create discriminatory or unjust consequences. Similarly, Li 
et al. (2022) consider fair decisions as ones that do not systematically 
discriminate against individuals or groups of specific attributes (e.g. 
gender, race, learning profile). Islam et al. (2022) also consider a fair 
decision to be free from favoritism or prejudice towards individuals or 
groups based on their inherent or acquired characteristics. A more 
detailed descriptive analysis of the literature is also made. In (Kobis & 
Mehner, 2021) and (Kong et al., 2023), a review of the literature is 
presented surrounding fairness. Marcinkowski et al. (2020) pinpoint two 
facets of fairness. While factual fairness refers to objectively measurable 
features, perceived fairness is a construct that relates to individual 
perceptions. 
Equally important is defining practices that lack fairness and lead to 
bias. Ungerer and Slade (2022) pose that AI systems may be founded on 
unfair processes and data. Similarly, Islam et al. (2022) warn that data 
bias and algorithmic bias are the primary contributing factors for 
fairness-related risks in AI-based decision-making. In (Zembylas, 2023) 
the potential biases of algorithms towards marginalized groups are 
noted. Baker and Hawn (2022) highlight the diversity of biases existing 
in higher education. Examples include but are not limited to “the sta-
tistical biases of measurement and error to imbalances in how well a 
model performs across groups, to systematic skew in results, to disparate 
impacts and discrimination as model results are interpreted and 
applied.” (p. 4). A key area of challenge towards achieving fairness is 
predictive modeling. Afrin et al. (2022) explore how some predictors 
used by intelligent algorithms may be unfair to an individual or group in 
various situations. Deho et al. (2022) also note that issues of discrimi-
nation may exist by predictive models based on protected attributes of 
students. 
Various bias mitigation and fairness literacy approaches are also 
provided. Ten´orio et al. (2023) suggest avoiding reliance on an unfair 
concentration of AI knowledge. Prinsloo and Slade (2017) offer various 
approaches to achieving fairness, namely “(1) a utilitarian approach (2) 
a rights approach (3) a fairness or justice approach; (4) the 
common-good approach; and (5) the virtue approach” (p. 5). Casacu-
berta and Guersenzvaig (2019) focus on Dreyfus’ account of ethical 
expertise which assumes the longer a practice the more a person be-
comes an expert in that practice. The authors present a scale for ethical 
expertise (5 levels) ranging from novice to expert. Gorur et al. (2020) 
focus on fairness literacy and having graduates understand fairness, bias 
privacy, discrimination sustainability, inclusivity, and social justice. 
3.1.1.2. Technical definitions of fairness or bias. In technical terms, 
fairness is often captured during data handling or modeling stages 
through statistical and mathematical means. Jiang and Pardos (2021) 
focus on metrics to measure group fairness. They pose that strategies to 
mitigate algorithmic bias can be designed and implemented in the three 
stages of a typical machine learning pipeline, namely dataset construc-
tion, model training, and inference. 
From a statistical and more broadly mathematical perspective, 
Mashhadi et al. (2022) use batch classification and parities and assume 
hierarchies of unfairness exist that need to be undone. Quan et al. (2023) 
note that researchers may give a variety of definitions of fairness ac-
cording to a specific application scenario. Examples include but are not 
limited to statistical parity, classifiers satisfying equal opportunity, ac-
curacy parity, and joint parity. Kim and Cho (2022) present three main 
definitions for fairness: parity, equalized opportunity, and equalized 
odds. Fang et al. (2020) also share several mathematical efforts done to 
achieve fairness such as disparate impact and individual fairness. 
Accounts of bias in technical terms are also noted. Barbierato et al. 
(2022) present three types of bias: (1) preexisting bias (2) technical bias 
(3) emergent bias. The authors further share that fairness problems may 
be addressed by: (1) pre-processing techniques (2) in-process methods, 
and (3) post-process approaches. Shulner-Tal et al. (2023) consider that 
algorithmic decision-making systems (ADMSs) are to be viewed as black 
boxes and tend to have no agreed-upon fairness solutions. To improve 
fairness the authors suggest using techniques such as explainability, or 
fairness certification that can help link perceived fairness and compu-
tational fairness. 
In a more user-interactive view, Segal-Halevi et al. (2020) make use 
of ordinal algorithms that require the users to rank and report a total 
order among items. Similarly, Pereira et al. (2021) focus on SHapley 
Additive ex Planations or SHAP which explains the predictions of ma-
chine learning models. The authors pose that to calculate the feature 
contributions fairly, SHAP keeps fairness properties called additivity, 
missingness, and consistency. 
3.1.2. Accountability 
Accountability is the second most noted FATE term in the reviewed 
studies (n= 9). Below we summarize definitions presented surrounding 
achievement accountability that is presented in both descriptive and 
technical terms. 
3.1.2.1. Descriptive definitions of accountability. In descriptive terms, 
Fig. 4. Fairness (F), Accountability (A), Transparency (T), and Ethics (E) terms 
from the reviewed studies and whether presented in Descriptive (D) or Tech-
nical (T) terms. 
B. Memarian and T. Doleck                                                                                                                                                                                                                  

--- Page 6 ---

Computers and Education: Artificial Intelligence 5 (2023) 100152

accountability in the context of AI and higher education is denoted as 
either the set of preventative or mitigation strategies that make owners, 
designers, or users of artificially intelligent algorithms held responsible. 
Higher education may utilize algorithms or human committees/stake-
holders for strategic decision making so there may be many levels of 
accountability. 
Bezuidenhout and Ratti (2021) consider accountability synonymous 
with explicability and so any program that is capable of being explained 
is considered accountable. Shin et al. (2022) consider the concept of 
accountability as the measure aimed at holding the providers of auto-
mated decision systems responsible for the results generated by their 
programmed decision-making. More specifically, Beerkens (2022) views 
the notion of accountability from the perspective of an educational 
audience and suggest that the utility of specificity over simplicity of 
indicators depends strongly on an intended educational audience (e.g., 
internal vs. external monitoring). Prinsloo and Slade (2017), on the 
other hand, set accountability’s focus between the institution and the 
student. They suggest various factors, often legal, contribute to 
accountability such as foreseeability of harm to the plaintiff; degree of 
certainty of harm to the plaintiff; burden upon the defendant to take 
reasonable steps to prevent the injury, and so on. 
Pagallo (2017) highlights that it is far from clear who should be held 
accountable, i.e., either the producer or manufacturer of the AI system 
or the supplier of the data. Similarly, Ungerer and Slade (2022) consider 
a lack of accountability associated with systems built around algo-
rithmic models. Part of the reason for a lack of accountability could be 
due to the conceptualization of AI programs. Bearman et al. (2022) note 
a discourse of altering authority is creating accountability implications 
by AI. Yet, there are diverse views on whether AI or humans or both 
should be made accountable. For example, Kong et al. (2023) note that 
AI applications are not used to manipulate people. Instead, humans are 
the ones to make decisions and should be made accountable. Whereas Li 
et al. (2022) suggest that both AI and humans should be accountable 
when needed. 
3.1.3. Transparency 
Transparency is the third most noted FATE term in the reviewed 
studies (n= 7). Note that transparency can be seen through many lenses, 
two of which as presented in the Merriam-Webster dictionary are 
(Merriam-Webster, 2023): the ability for the algorithm to explain itself 
and its inner workings vs. the ability of the algorithm to make itself 
seamless and make objective outcomes of the users more apparent. 
Below we summarize definitions surrounding the achievement of 
transparency that are presented in both descriptive and technical terms. 
3.1.3.1. Descriptive definitions of transparency. In descriptive terms, 
transparency in the context of AI and higher education is often denoted 
at both a high- and low-level. At a high-level, transparency concerns 
making institutional policies and their motives and outcomes trans-
parent. At a low-level, algorithms used for teaching AI or utilizing AI in 
higher education should become transparent (in either mathematical or 
laymen’s terms) to their users. 
An important transparency consideration concerns the students. 
Ungerer and Slade (2022) consider student data’s transparency, consent, 
and the right to seek redress. The authors suggest making AI systems 
more transparent and data sources available for audit like Europe’s 
General Data Protection Regulation (GDPR) (Regulation, 2018). Pagallo 
(2017) emphasizes the need to provide the data subjects with all the 
information necessary to ensure fair and transparent processing. More 
broadly, Kong et al. (2023) pose that the usage, merits, and drawbacks of 
AI applications need to be clearly stated as a way to address trans-
parency. Gorur et al. (2020), on the other hand, necessitate the devel-
opment of ethical codes and accountability measures as a way of 
transparent communication. 
These concerns are valid as there is a general decline in transparency 
with big data and compressed data that often are taken by AI algorithms 
in higher education (Beerkens, 2022). Further, there may also be a need 
for ways to interpret the predictive model’s decision for extracting an 
explainable, transparent model for AI in Education (Pereira et al., 2021). 
3.1.3.2. Technical definitions of transparency. In technical terms, trans-
parency is captured through designs that make black-box models more 
transparent. Litman et al. (2021) suggest using a hybrid model by 
combining a neural network and a feature-based model. 
3.1.4. Ethics 
Ethics is the least noted FATE term in the reviewed studies (n= 7). 
Note that ethics is relatively broader and considered more as an um-
brella term in the reviewed studies when compared to fairness, 
accountability, and transparency. As such studies may note the 
achievement of fairness, accountability, transparency, and other objec-
tive terms to contribute to the achievement of ethics. 
3.1.4.1. Descriptive definitions of ethics. In descriptive terms, ethics in 
the context of AI and higher education concerns an array of factors, 
given that ethics is a broad concept. For example, Williams et al. (2020) 
explore ethics and moral philosophy; concepts to analyze applications of 
AI and Robotics; computational approaches to moral decision-making; 
psychological theories of moral decision-making and blame; and, 
methods for experimentally investigating ethical issues. While Beerkens 
(2022) examines the privacy of people’s profiles, ownership, and data 
security. Overall studies highlight the need to make ethical issues more 
apparent and understandable. Gorur et al. (2020) share that all 
Australian engineering undergraduate courses are mandated to have an 
ethics component Ten´orio et al. (2023) explore the need for in-
terventions to raise awareness of AI’s ethical issues. Ungerer and Slade 
(2022) make the case that governance measures, supportive systems and 
organizational structures, and clear policy guidelines are necessary with 
inputs from students and staff. While Kong et al. (2023) reflect on ethical 
principles that were built from the Belmont Report: “(1) the use of AI 
should not violate human autonomy; (2) AI’s benefits should outweigh 
its risks; and (3) AI’s benefits and risks should be distributed equally 
(Autonomy, beneficence/nonmaleficence and fairness)” (p. 3). 
3.1.4.2. Technical definitions of ethics. In technical terms, Segal-Halevi 
et al. (2020) use stochastic dominance or SD set extension to choose 
from ethical theories. 
3.2. FATE investigated in the reviewed articles 
A count summary of Fairness (F), Accountability (A), Transparency 
(T), and Ethics (E) terms from the reviewed studies and whether 
examined through a quantitative/quant (Likert style survey, empirical 
analysis) or qualitative/qual (literature review, reflective analysis, or 
open-ended interviews and focus groups) lens is presented in Fig. 5. We 
find studies on fairness and ethics to take the lead and accountability 
and transparency to be explored at a much-lowered rate in the reviewed 
studies. 
Note that in the previous section, we shared definitions of each of the 
FATE terms present in each article. However, each article predominantly 
focused on and studied one FATE term. Hence the number of studies 
reported for each of the FATE terms for RQ1 (any FATE term defined in 
each study) and RQ2 (only the main FATE terms examined in each 
study) are different. Here we aim to present a summary of studies con-
ducted, including their methods, results, potential limitations, and 
future work for each of the FATE terms. 
3.2.1. Fairness 
Fairness is the most studied FATE term in the reviewed studies (n=
16). 
B. Memarian and T. Doleck                                                                                                                                                                                                                  

--- Page 7 ---

Computers and Education: Artificial Intelligence 5 (2023) 100152

3.2.1.1. Qualitative studies on fairness. Taking insight from users 
through open-ended questions has been used to identify benefits and 
challenges with achieving algorithmic fairness in AI programs. In 
(Mashhadi et al., 2022), a survey on six open-source tools is made, 
namely: Aequitas, AI Fairness 360, Dalex, Fairlean, Responsibly, and 
What-If-Tool. The authors highlight this study is focused on fairness 
assessment and not mitigation and allow an analysis of programs that 
utilize embedded visualizations. Results of the qualitative review and 
observations of four focus groups present a need for transparency, 
dataset integration, and interactivity. The study may be limited in the 
datasets 
used 
and 
primarily 
examined 
tools 
used 
by 
the 
human-computer interaction community. Future work can use more 
diverse datasets and discuss the implications of tool-oriented group 
work for the assessment of fair AI algorithms. 
A literature review by Baker and Hawn (2022) shares the concrete 
impacts of algorithmic bias in education. They raise an important 
concern that bias does not only exist in the problems we are aware of. 
Rather we need to look for evidence of unknown or poorly understood 
biases as well. While the review offers several influencers to algorithmic 
bias, it does not share mitigation strategies in detail. For a better un-
derstanding of algorithmic bias, the authors suggest that future work 
include data sets that contain a wide array of demographic variables and 
have both macro and micro-outcomes as label variables. 
3.2.1.2. Quantitative studies on fairness. To examine the fairness of al-
gorithms and AI programs, quantitative studies are conducted in various 
areas of the data science pipeline and stages of algorithmic decision- 
making. One area examined is the sources and datasets and their asso-
ciated biases. Barbierato et al. (2022) generate a synthetic data set with 
a controlled amount of bias and fairness using structural equation 
modeling. Their approach enables the development and evaluation of 
the effectiveness of bias mitigation approaches for machine learning 
models and classifiers. Their study, like many others conducted in this 
area, however, is limited in using a positivist approach for character-
izing constructivist terms of fairness and bias. Nevertheless, the authors 
find the use of data sets that have a controlled amount of bias to be 
useful in making fairness-aware developers and programs. They further 
suggest creating an integrated tool for the future generation of synthetic 
data sets. 
Another area is the deployment of courses or interventions that can 
collect participants’ (often students’) outcomes and experiences. In 
(Shulner-Tal et al., 2023), an online between-subject experiment is 
conducted by employing a case study of a simulated AI-based recruit-
ment decision-support system with a focus on three aspects: system 
characteristics, 
personality 
characteristics, 
and 
demographic 
characteristics. The authors offer a framework for predicting a layper-
son’s perception of the fairness of the explanations. Yet their study may 
be limited by the influence of the scenario and the self-reported nature of 
fairness perception. So, the authors aim to extend and generalize the 
framework for future work. 
Afrin et al. (2022) aim to identify how students perceive different 
success predictors and the reasons behind their perceptions. The authors 
use three scenario-based user studies comprising students from univer-
sities in three different countries and noted the large perception changes 
across these different scenarios. Findings show that students’ perceived 
responses are complex and diverse which may suggest that user-specific 
inputs should be accounted for in the development phase of 
fairness-aware algorithms. Future work can thus include a perception 
analysis for different demographics to analyze the individual shift in 
perception under various scenarios. 
Marcinkowski et al. (2020) use cross-sectional survey data from a 
large German university to evaluate if the students’ assessments of 
fairness differ concerning algorithmic decision-making vs. human 
decision-making within the higher education context. They pose two 
types of fairness, namely distributive and procedural. Distributive fair-
ness, as the authors denote, refers to the fair distribution of resources. 
Procedural fairness relates to the process, or in technical terms, the data 
and mechanism used to achieve the outcome. The results of the survey 
suggest that participants evaluated algorithmic decision-making higher 
than human decision-making in terms of both procedural and distribu-
tive fairness. The study is susceptible to the limitation of self-selected 
samples (e.g., excluded students who were not on campus during 
study time) and so future work can introduce the learning module to 
more courses. 
Pereira et al. (2021) also design a long-term study using very 
fine-grained log data of a large sample size which were collected from 
the first two weeks of a computer science course. Using this data, the 
authors then construct a predictive model using a game-theory-based 
framework (SHAP). Their work enables the development of an 
explainable machine-learning pipeline that showed significant benefits 
from using fine-grained data. 
Islam et al. (2022) develop a module that includes lectures and 
hands-on exercises and uses state-of-the-art and open-source bias 
detection and mitigation software on real-world datasets. Results of the 
Likert-style survey show that the curricular module can help understand 
where bias and discrimination exist and their associated implications in 
automated decisions. The authors aim to further deploy the module in 
other courses and institutions. 
A few studies examine AI fairness in an educational grading context. 
Litman et al. (2021) compare automated scoring systems using algo-
rithmic fairness rather than scoring accuracy or model transparency. 
They find that different automated scoring systems present different 
types of biases, spanning students’ gender, race, and socioeconomic 
status. Their study assumes human scores as the gold standard, which 
may not always be true. The authors pose future work to identify bias in 
human scores by examining differences with a consistent and replicable 
automated scoring system. Jiang and Pardos (2021) propose strategies 
during the data processing stage, the model training stage, and the 
inference (prediction) stage of the grade prediction model to improve 
group fairness while maintaining overall accuracy. The authors 
conclude that (1) adversarial learning is best for fairness, (2) an 
equity-based strategy is effective (3) and that there is a need to account 
for different race groups. 
Tuning the classification models during the pre-processing, pro-
cessing, or post-processing stage is another area covered by the reviewed 
studies. Kim and Cho (2022) attempt to preserve the characteristics of 
the original data while excluding the information about the features 
causing bias. The authors propose a pre-processing approach based on 
information theory that avoids collision in dual optimization, where the 
latent space is divided into two subspaces. Their proposed method can 
learn the representation independent of the classifier including deep 
Studies (n=33) 
F (n=16)
Qual (n=2)
Quant (n=14)
A (n=3)
Qual (n=3)
Quant (n=0)
T (n= 2)
Qual (n=0)
Quant (n=2)
E (n=12)
Qual (n=8)
Quant (n=4)
Fig. 5. Fairness (F), Accountability (A), Transparency (T), and Ethics (E) terms 
from the reviewed studies and whether examined through a quantitative (Likert 
style survey, empirical analysis) or qualitative (literature review, reflective 
analysis, or open-ended interviews and focus groups) lens. 
B. Memarian and T. Doleck                                                                                                                                                                                                                  

--- Page 8 ---

Computers and Education: Artificial Intelligence 5 (2023) 100152

learning. This may be because the bias has the lowest value when the 
output of the encoder is input to classifiers. Their proposed method is 
further verified with two applications as well as well-known benchmark 
datasets. However, since the process is dual optimization, the approach 
can lead to unstable learning. Their future work aims to find a way to 
reduce bias differences more stably. 
Fang et al. (2020) examine fair groups based on disparate impact to 
improve fairness in prediction outcomes. The authors find using random 
forests in combination with their algorithm can help achieve the best 
result in the fairness of prediction outcomes. However, the authors 
caution that the identification of protected vs. unprotected groups may 
contain biases that get fed into the algorithms. 
Segal-Halevi et al. (2020) focus on a term coined as Diminishing 
Differences or DD. They assume that the utility difference between the 
best item and the second-best item is at least as large as the utility be-
tween the second-best and the third-best, and so on. 
Quan et al. (2023) introduce a fair classification algorithm based on 
similarity representations and correlation constraints. Their model 
shows a better tradeoff between accuracy parity and joint accuracy than 
existing models. Yet, they find the value of hyperparameters cannot 
determine whether the final classification result is fairer or more accu-
rate. So, the authors aim to analyze the correlation between indicators 
and sensitive information in other aspects of the classification results. 
Deho et al. (2022) examine fairness outcomes when the protected 
attributes from the learning analytics models are excluded. The authors 
perform empirical evaluations using 3-year dropout data for a particular 
program in a large Australian university. They find that excluding/in-
cluding the protected attributes has a marginal effect on predictive 
performance and fairness. However, if a protected attribute is correlated 
with the target label and proves to be an important feature, then their 
inclusion or exclusion would affect the performance and fairness and 
vice versa. The study is limited in the smallness of the dataset used. So 
future work needs to conduct the analysis on a larger dataset and across 
many programs. 
Li et al. (2022) develop fair-logistic regression and compare it with 
fairness-unaware AI models: Logistic Regression, Support Vector Ma-
chine, and Random Forest. Their method generates desirable predictive 
accuracy while achieving better fairness. For future work, the authors 
suggest the educational community adopt a methodological shift to 
achieve accurate and fair AI to support learning and reduce bias. 
3.2.2. Accountability 
Accountability is only studied by three studies qualitatively. 
3.2.2.1. Qualitative studies on accountability. Two studies provide a 
description of considerations surrounding the achievement of account-
ability in AI and higher education. Beerkens (2022) presents the po-
tential and quality assurance best practices for employing big data in 
education. The author suggests that the high volume of data, its speed of 
accumulation, and related analytical techniques require examining their 
ethical and policy challenges. Similarly, Pagallo (2017) reflects that 
normative challenges of AI entail different types of accountability that 
go hand-in-hand with choices of technological dependence, a delegation 
of cognitive tasks, and trust. The authors suggest that legislators must 
tackle three different kinds of challenges, namely the specific features of 
AI technology, the interaction between competitive regulatory systems, 
and how lawmakers can address such challenges at a meta-regulatory 
level. Some legal techniques, such as the rules of experimental feder-
alism or the implementation neutrality-approach, are proposed as 
mitigation strategies. Yet the authors highlight the need to discuss how 
the future can be legally regulated considering AI programs in higher 
education. 
Bearman et al. (2022) conduct a literature review and share key 
three research foci that need to be addressed. They are: (1) the language 
of AI; (2) issues of accountability and labor; and, (3) the implications for 
teaching and learning contexts beyond single innovations. Future work 
thus needs to examine how AI-mediated technologies shift account-
ability and ways AI can influence pedagogical relationships. 
3.2.3. Transparency 
Transparency is only studied by two studies quantitatively. 
3.2.3.1. Quantitative studies on transparency. Sosnovsky and Brusilovsky 
(2015) use top-based personalization for adaptive educational systems 
to enhance transparency using a specific software called Quiz Guide. The 
authors find that the adaptive system Quiz Guide based on 
coarse-grained topics has performed extremely well. Yet the work fo-
cuses on one subject domain, set of topics, type of content, system, and 
adaptation technique. 
Shin et al. (2022) employ multiple-group equivalence methods to 
compare two-group invariance and the hypotheses concerning differ-
ences in the effects of algorithmic literacy. The authors suggest that 
considering the industry’s decreasing credibility, work can be done by 
the industry to share what positive engagement with algorithm plat-
forms would look like and the trust of service more generally. 
3.2.4. Ethics 
Ethics is studied by twelve studies, eight qualitative, and four 
quantitative means. 
3.2.4.1. Qualitative studies on ethics. Review work done on AI ethics 
present that much room is needed for improvement. Ten´orio et al. 
(2023) conduct a bibliometric review of studies concerning the AI lit-
eracy field. The authors find AI is an evolving field which may make its 
ethics also evolve and have weak international collaboration. So, there is 
a need to develop the AI ethics literacy field further. Kobis and Mehner 
(2021) also conduct a review of articles. They initiate a discourse that 
enables discussing relevant ethical questions in both face-to-face and AI 
mentoring environments. 
Various aspects of AI programs are still hidden, such as their trans-
parency and coloniality more specifically. The chapter presented by 
Ungerer and Slade (2022) explores the ethical concerns surrounding the 
impact of AI in education. They suggest AI systems need to be made 
easier to explain, and more auditable and transparent. In (Zembylas, 
2023), the author reflects on decolonial interventions needed in the 
context of AI and higher education. The theorization leads to a decolo-
nial conceptual framework where both decolonial practices and strate-
gies are endorsed. However, diverse contexts (e.g., Africa, and South 
America) are not studied. Future work requires the creation of spaces 
that allow critical questioning of the coloniality of AI. 
One way to achieve ethics is to make students informed of data, 
privacy, and informed consent. Prinsloo and Slade (2017) conduct 
qualitative interpretative case studies on tutor contracts of two different 
distance learning institutions. The authors find more work needs to be 
done to know about students’ dispositions, learning behaviors, and risk 
profiles. Further, the responsibility resulting from learning analytics has 
yet to be understood. Gorur et al. (2020) present early findings on the 
ethics courses of Australian Computer Science undergraduate degrees 
(12 courses). They find most courses focus on micro-ethical concepts like 
professionalism and there is a lack of explicit macro-ethical agendas. 
Future work can examine how macro-ethics may be effectively incor-
porated. Bezuidenhout and Ratti (2021) present an example of how the 
‘micro-virtue ethics’ approach could be practically taught to data sci-
ence students. The authors recommend: “ground teaching strategies 
within a virtue ethics framework, and to think about ethical training as a 
way to help students and practitioners to cultivate two main virtues (i.e., 
moral attention and appropriate extension of moral concerns)” (p. 14). 
Casacuberta and Guersenzvaig (2019) emphasize that a purely 
symbolic, conceptual approach to give an AI some ability to make 
ethical judgments is not enough. Rather, AI also needs multiple 
B. Memarian and T. Doleck                                                                                                                                                                                                                  

--- Page 9 ---

Computers and Education: Artificial Intelligence 5 (2023) 100152

perspectives in a pre-reflective way. As such both code and training 
examples need to be open and transparent. Further, committees of ex-
perts should exist as well as common citizens to reflect on everyday 
ethics. The authors summarize that future work will need: “(1) Ethical 
declarative concepts, (2) Generate ability for autopoiesis, (3) capture 
common pre-reflective ethical judgments, (4) define situations and its 
constitutive elements and describe the importance of these elements and 
their mutual relationships” (p. 5). 
3.2.4.2. Quantitative studies on ethics. Williams et al. (2020) conduct an 
experimental ethics-based curricular module for an undergraduate 
course on Robot Ethics. Their study is small in sample size and so no 
quantifiable benefit of ethics learning outcomes achievement is found. 
Yet the module provided additional learning opportunities. The authors 
distinguish between moral learning and learning and suggest making 
moral learning a part of every learning experience. 
Kong et al. (2023) conduct an ethical evaluation survey with a focus 
on autonomy, beneficence, and fairness. The results of the survey show 
that students considered all these three factors important as well as 
transparency and privacy. The work enabled the integration of AI ethics 
in project work and not in an after-the-fact manner. 
Javed et al. (2022) use Latent Dirichlet Allocation (LDA) which is a 
generative probabilistic topic model. Their study aims to showcase 
topics in teaching ethics in AI courses and where such courses are being 
taught, by which individuals or groups. They further code the level of 
cognitive complexity of each course using Bloom’s taxonomy. The au-
thors find numerous significant misalignments between the learning 
objectives and instructional design set for teaching such courses. 
Lucic et al. (2022) present work on a one-month, full-time course 
based on examining ethical issues in AI using reproducibility as a 
pedagogical tool. The authors illustrate that reproducibility is not only 
paramount to good science in general but is also a fundamental 
component of Fairness, Accountability, Confidentiality, and Trans-
parency in Artificial Intelligence (FACT-AI) at the University of 
Amsterdam. Authors suggest identifying where the lack of reproduc-
ibility in this domain area is coming from and center the course projects 
on the evaluation. 
4. Discussion 
This study aimed to summarize definitions of Fairness, Account-
ability, Transparency, and Ethics (FATE) in the context of Artificial In-
telligence (AI) and higher education, We conducted a systematic 
literature review of 33 articles and found that there is a tendency to 
describe FATE more in descriptive rather than technical and study FATE 
more in quantitative rather than qualitative means. Fairness is found to 
be the most represented and studied term in the reviewed studies and 
more of a technical definition of Fairness is presented in the literature. 
The scarcity of work done on accountability and transparency, the all- 
encompassing characteristics of ethics, and the blurry explanations of 
fairness and bias warrant more discussions and a clearer explanation and 
examination of all FATE terms. 
The growing ethical concerns in the field of AI have led to more 
thorough investigations of the morality of AI in contexts such as edu-
cation (Raji et al., 2021). Yet there is still ambiguity around what con-
stitutes ethical AI in both technical and general terms (Jobin et al., 
2019). There is a need to explicitly consider issues such as fairness, 
accountability, transparency, and so on (Holmes et al., 2021). Specif-
ically, what is known as FATE or Fairness, Accountability, Transparency, 
and Ethics is becoming a cornerstone of ethical considerations in AI 
education (Inuwa-Dutse, 2023; Woolf, 2022). How technology may 
change the educational landscape, the impact of the different concep-
tions of technology, and the relationship between humans and AI are 
examples of areas explored in the literature. Yet we find minimal work in 
conducting a systematic review of the FATE terms in higher education. 
Our systematic review offered an examination of the definitions of FATE 
terms and types of studies conducted in the literature. Our review thus 
contributes to a shared understanding of FATE terms and types of studies 
conducted. 
Our systematic review has further helped us synthesize the literature, 
offer limitations, and suggest recommendations for future research in 
achieving each of the FATE terms. A summary of our synthesis based on 
the reviewed studies is presented below. 
4.1. Fairness challenges and recommendations 
A repository of working examples of tools is deemed useful (Mash-
hadi et al., 2022). This idea also motivates exploring a repository of 
FATE criteria (e.g., for fairness). To achieve this though we may need to 
reach a consensus on what is fair and what is unfair and whether this 
view is binary or fuzzy. Moreover, metrics, thresholds, and units need to 
be critically examined and defined for a repository of criteria sur-
rounding FATE terms. 
It is known that various demographic factors contribute to the level 
at which an individual or group may face bias in educational settings 
(Baker & Hawn, 2022). We often take bias as an individual’s 
under-represented minority or URM group. However, an individual may 
belong to multiple URM groups or face bias differently on different as-
pects of their representation over time. Therefore, a causal formulation 
and exploration of bias may be needed. 
Understanding bias may start from the intended addition of bias 
factors and seeing their impact on data lifecycle, AI algorithms, and 
decision-making processes (Barbierato et al., 2022). Yet we may need to 
be cautious and not take all perturbations in datasets as an issue. Issues 
may also go beyond individuals, groups, or policies doing bias or un-
doing fairness inappropriately. 
While AI advancements are best understood by the experts and their 
creators, AI by and large requires lay people’s understanding of AI 
programs (Shulner-Tal et al., 2023). This is because laypeople will be the 
main consumers and the ones most affected by AI programs. As such 
fairness, transparency, and explainability of AI programs need to 
appropriately be translated for and understood by laypeople. 
Fair predictors of student success can be identified by the students 
themselves (Afrin et al., 2022). Yet perceptions are also a result of stu-
dents’ upbringing and culture and so different countries for example 
may have different perceptions of fair predictors. The most and least 
sought-after fair predictors of student success may thus change from one 
scenario to another and have a rather transient characteristic. 
Students at a German university scored algorithmic decision-making 
higher than humanistic decision-making on both the fair distribution of 
resources and data and mechanisms used to achieve a desired outcome 
(Marcinkowski et al., 2020). This perception, as the authors suggest, 
may have a negative impact on making protests of the algorithmic 
decision-making system. Adding to this issue is that if a protest is made, 
it is still administered through a human, and thus humanistic 
decision-making becomes entangled with adjustments made to the 
algorithmic decision-making system. 
When developing predictive models, a major tradeoff is to make the 
model both accurate and transparent, as naturally when accuracy in-
creases the transparency and understandability of the program decrease 
for humans (Pereira et al., 2021). Using explainable models and tools to 
make models understandable is one way to address this tradeoff. Yet, 
still, more work needs to be done to make explainable models to be fully 
descriptive of how features contribute to the target variable, rather than 
the prediction. 
Educating newer generations about bias and discrimination is crucial 
(Islam et al., 2022). But notions of bias and discrimination are evolving 
with rapid technological, computational, and algorithmic advancements 
in the field of AI. Approaches are thus needed to stay current with rapid 
changes yet offer a universal and timeless set of fair attributes and 
conduct. 
B. Memarian and T. Doleck                                                                                                                                                                                                                  

--- Page 10 ---

Computers and Education: Artificial Intelligence 5 (2023) 100152

Comparing automated scoring systems relative to humanistic- 
decision making may be another practice to identify more appropriate 
practices (Litman et al., 2021). However, the reality is that 
decision-making by neither humans nor AI is the gold standard. We thus 
may need to examine the fairness impact of having AI vs. human vs. 
human and AI assessors in the scoring process. 
Analysis fairness has been done through predictive models that lack 
sensitive attributes (Deho et al., 2022; Jiang & Pardos, 2021). Adver-
sarial machine learning approach, for example, tricks machine learning 
models by providing deceptive input. The deception could for example 
be surrounding sensitive attributes such as race, and sex, to trick the 
model and prevent it from making biased decisions. The key goal is to 
turn off the correlation between the model’s detection of sensitive at-
tributes and their use for model prediction (Jiang & Pardos, 2021). Yet 
masking sensitive attributes may not be feasible at all times, especially 
when trying to achieve fairness within a sensitive group (e.g., 
low-income students) with an attribute that has several levels (e.g., 
parent’s level of income). 
Statistical approaches are examined to reduce bias caused by fea-
tures such as age, race, and gender. The analysis may work with the 
probabilities of a sample with and without special features to show up 
evenly or both have favorable outcomes (Kim & Cho, 2022). Such 
analysis may concern groups of well versus under-represented students 
and seek fairness at a group level (e.g., an equal number considered from 
each group) and not concern about fair allocation at an induvial level. 
AI algorithms may have been coded with the good intention to 
allocate resources fairly among individuals and groups (Fang et al., 
2020). Yet achieving such goals may contain errors and unintentionally 
allocate resources to those who are not in urgent need. As a result, the 
algorithms themselves create new dimensions of unfairness that may 
have repercussions not easily rectified. 
Fair allocation of resources, prediction, and accuracy parity of al-
gorithms can also be studied from a mathematical perspective (Li et al., 
2022; Quan et al., 2023; Segal-Halevi et al., 2020). Such analytical work, 
however, may run the risk of confusion and incorrect interpretation by 
laypeople. Accordingly, how accurately and appropriately such analyt-
ical work can be reformulated for lay people whether through humans or 
explainable models, is an area that needs more attention. 
4.2. Accountability challenges and recommendations 
Big data may seem promising to achieve personalized student 
learning, counseling and services, organizational management, and 
external governance (Beerkens, 2022). Yet we must note that big data’s 
real estate at present is a collection of measures that might be collected 
sporadically, and so in reality, much more computational power and 
technology may be needed to utilize data for promising avenues. 
Accountability of AI systems needs to be handled at both a micro and 
macro level and further account for the lawmakers, regulatory system, 
and social systems of morality in general (Pagallo, 2017). AI as a tech-
nique and tool is more universal. Lawmaking and regulations are rather 
distributed and local to geographical regions. There is as such a chal-
lenge to align AI developments with appropriate policies at the same 
time across different geographical regions. 
In a sense, when teachers use AI in their work they come to shift or 
share accountability with AI programs (Bearman et al., 2022). This ne-
cessitates a clear understanding of both the teacher and AI of one 
another to take responsibility as a united force or team. When there is a 
lack of understanding, either the teacher or AI may take the blame or 
reward when in fact it was not warranted. 
4.3. Transparency challenges and recommendations 
Personalized facilitation of student learning requires a system to both 
model students and adapt based on students’ input to the system. Topic- 
based modeling is one way to achieve such goals (Sosnovsky & 
Brusilovsky, 2015). Yet, the complexity of analysis may increase when 
shifting the study from one subject domain, set of topics, content, sys-
tem, and adaptation techniques to multiple. Conventions are thus 
needed to outline the logistics and practices for student modeling and 
adaptation under diverse contexts. 
Algorithmic literacy today not just requires coding knowledge but 
also knowledge to critically examine FATE issues behind algorithms 
(Shin et al., 2022). Perceived transparency may be directly related to the 
trust between users and algorithms. Yet transparency may go beyond 
what users perceive and include how algorithms trust humans and 
naturally characterize transparency. 
4.4. Ethics challenges and recommendations 
Research in AI literacy is predominantly administered by each 
country individually, without much international collaboration. 
Further, AI literacy has gained much more dissemination in recent years 
and since 2018, with conference proceedings being the usual venue 
(Ten´orio et al., 2023). Given the computer-assisted nature of the field 
and the availability of the cloud, it is quintessential that researchers 
unite and change the competitive landscape of the field to a collabora-
tive one. We know many of the issues in the FATE of AI lie in the 
preparation of databases, data sharing, and consent. With international 
collaboration, we may still see a rapid rate of dissemination with a more 
thorough discussion of how each approach unfolds under different 
contexts and cultures. 
Both AI-supported mentoring and AI education need an exploration 
of classical ethical questions (Kobis & Mehner, 2021). Yet, each domain 
may also entail ethical principles of its own. Reformulating such prin-
ciples into actionable evaluations made by AI programs and automation 
needs to be fulfilled shortly. This is to make our formative understanding 
of ethical principles into quantifiable strategies that can be undertaken 
by AI programs. 
The challenge in achieving ethics with AI in education is not so much 
about what should be addressed. We already know a host of areas where 
ethics is lagging and that AI is intertwined with society, culture, ideol-
ogies, politics, economy, and industry (Ungerer & Slade, 2022). The 
challenge may be more so related to creating the capacity to make 
ethical changes or integrations that are appropriate at the right times. 
Another ethical consideration is to characterize colonial vs. decolo-
nial AI programs (Zembylas, 2023). To achieve this, we may need to use 
anti-bias heuristics and frameworks. By doing so we can analyze 
everyday decision-making against best practices and get to label pro-
cesses that are fair vs. need further improvement. 
The obligation to act is needed at every level of higher education (e. 
g., program, course) to protect student privacy and ethical manipulation 
of student data and resource allocation (Prinsloo & Slade, 2017). Yet the 
disconnects and even contradicting messages at different levels of higher 
education may change the face of the obligation to act. AI programs can 
be used to make the disconnections and contradictions apparent and 
create a dialogue on how to make the obligation to act achieve similar 
outcomes whether done in a bottoms-up (e.g., course to program) or 
top-down (e.g., program to course) approach. 
Analysis of 12 Australian universities shows various interpretations 
of ethics (e.g., macro vs. micro ethics) and appearances (e.g., teamwork, 
cyberspace regulation) to exist in course curricula (Gorur et al., 2020). 
Given the lack of a universal definition of ethics, it may be still unknown 
whether interpretations of ethics are appearing in curricula appropri-
ately. Professions may need more comprehensive and collective foun-
dations of ethics to understand one another better in the real-world 
industry. 
The utility of higher-level case studies for technical courses and 
concepts such as data science is questioned (Bezuidenhout & Ratti, 
2021). Yet it is known that a purely symbolic and conceptual approach 
for AI to make ethical judgments is not enough (Casacuberta & Guer-
senzvaig, 2019). This issue opened discussions on ways to embed ethics 
B. Memarian and T. Doleck                                                                                                                                                                                                                  

--- Page 11 ---

Computers and Education: Artificial Intelligence 5 (2023) 100152

into everyday choices when using the practice of data science. Micro 
ethics, which has been developed in the medical field as a way of 
emphasizing ‘micro-decisions’ that need to be made in an ongoing and 
frequent manner has shown promise in data science teaching and 
learning contexts as well. The documentation of such micro-ethics, like 
the medical field, may thus be needed. What might make micro ethics 
more complicated in data science as compared to the medical field, 
however, is that not just one underlying objective (i.e., saving humans’ 
lives) is present. 
Students can become part of AI education by serving as research 
participants (Williams et al., 2020) or contributing to reproducibility 
reports (Lucic et al., 2022). Doing so may allow students to feel and 
experience the role ethics play by observing their participation out-
comes. However, students’ understanding of ethics may need to go 
beyond research facilitation or robot supervision. As such a diverse set of 
experiential scenarios may need to be in place for the students to gain a 
deeper understanding of the complexities presented by ethics and AI 
education. 
Students who participated in an AI literacy program at a Hong Kong 
university demonstrated a heightened ethical awareness toward AI 
(Kong et al., 2023). The program prepares students to become confident 
in their ability to produce novel AI ideas, engage with AI as well as 
perceive the social impact of AI and the value of AI. An important 
realization of this program is in making students’ abilities and compe-
tencies aligned with their ethical development. Realistically, those with 
higher AI acumen also require a more developed understanding of 
ethical considerations when working with AI. 
Cognitive levels, such as the one by Bloom are used as a way to 
organize types of AI ethics courses (Javed et al., 2022). Yet indexing 
courses based on a set of Bloom’s verbs may distort the true cognitive 
demands of each course. Often the course learning objectives use a 
generic set of verbs and may not fully represent the course. 
4.5. Limitations of our work 
We found some limitations in our work. First, we only considered 
studies that had contextualized FATE on AI and higher education con-
texts and as such omitted including studies that may have explored FATE 
or AI and higher education in isolation. Also, we found many studies to 
briefly touch upon FATE terms but do not define or examine them as the 
main study. As such our review and findings are generalizable to the 33 
reviewed studies. 
5. Conclusion 
This systematic review paper of 33 articles aims to understand def-
initions and studies on Fairness, Accountability, Transparency, and 
Ethics (FATE) and AI in the higher education literature. Findings show a 
tendency to describe FATE more in descriptive rather than technical and 
study FATE more in quantitative rather than qualitative means. Fairness 
is found to be the most represented and studied term in the reviewed 
studies and more of a technical definition of Fairness is presented in the 
literature. The scarcity of work done on accountability and trans-
parency, the all-encompassing characteristics of ethics, and the blurry 
explanations of fairness and bias warrant more discussions and a clearer 
explanation and examination of all FATE terms. Future work may 
benefit from bridging the gap between laypeople and experts by linking 
descriptive definitions with technical ones as well as qualitative studies 
with quantitative ones. Moreover, future work can study accountability 
and transparency further and make the study of FATE terms more lon-
gitudinal, open-access, and reproducible. 
Funding 
This work was supported by the Canada Research Chair Program; 
and Canada Foundation for Innovation. 
Declaration of competing interest 
The authors declare that they have no known competing financial 
interests or personal relationships that could have appeared to influence 
the work reported in this paper. 
ACRONYMS 
Wo S 
Web of Science 
AI 
Artificial Intelligence 
FATE 
Fairness, Accountability, Transparency, and Ethics 
ADMS 
Algorithmic Decision-making Systems 
GDPR 
Europe’s General Data Protection Regulation 
SD 
Stochastic Dominance 
SHAP 
SHapley Additive ex Planations 
LDA 
Latent Dirichlet Allocation 
FACT-AI Fairness, Accountability, Confidentiality, and Transparency 
in AI 
References 
Adams Becker, S., Cummins, M., Davis, A., Freeman, A., Hall Giesinger, C., & 
Ananthanarayanan, V. (2017). NMC horizon report: 2017. Higher Education Edition.  
Afrin, F., Hamilton, M., Thevathyan, C., & Majrashi, K. (2022). Investigating perceptions 
of AI-based decision making in student success prediction. In 23rd international 
conference on artificial intelligence in education, AIED 2022, 13356 LNCS pp. 315–319). 
Springer Science and Business Media Deutschland Gmb H. https://doi.org/10.1007/ 
978-3-031-11647-6_60.  
Baker, R. S., & Hawn, A. (2022). Algorithmic bias in education. International Journal of 
Artificial Intelligence in Education, 32(4), 1052–1092. https://doi.org/10.1007/ 
s40593-021-00285-9 
Barbierato, E., Della Vedova, M. L., Tessera, D., Toti, D., & Vanoli, N. (2022). 
A methodology for controlling bias and fairness in synthetic data generation. Applied 
Sciences Basel, 12(9). 
Bearman, M., Ryan, J., & Ajjawi, R. (2022). Discourses of artificial intelligence in higher 
education: A critical literature review. Higher Education. https://doi.org/10.1007/ 
s10734-022-00937-2 
Beck, J., Stern, M., & Haugsjaa, E. (1996). Applications of AI in education. XRDS: 
Crossroads, ACM Magazine for Students, 3(1), 11–15. 
Beerkens, M. (2022). An evolution of performance data in higher education governance: 
A path towards a ‘big data’ era? Quality in Higher Education, 28(1), 29–49. https:// 
doi.org/10.1080/13538322.2021.1951451 
Bezuidenhout, L., & Ratti, E. (2021). What does it mean to embed ethics in data science? 
An integrative approach based on the microethics and virtues. AI & Society, 36(3), 
939–953. https://doi.org/10.1007/s00146-020-01112-w 
Borenstein, J., & Howard, A. (2021). Education., Emerging challenges in AI and the need 
for AI ethics. AI and Ethics, 1, 61–65. 
Bozkurt, A., Karadeniz, A., Baneres, D., Guerrero-Rold´an, A. E., & Rodríguez, M. E. 
(2021). Artificial intelligence and reflections from educational landscape: A review 
of AI studies in half a century. Sustainability, 13(2), 800. 
Casacuberta, D., & Guersenzvaig, A. (2019). Using Dreyfus’ legacy to understand justice 
in algorithm-based processes. AI & Society, 34(2), 313–319. https://doi.org/ 
10.1007/s00146-018-0803-2 
Chen, L., Chen, P., & Lin, Z. (2020). Artificial intelligence in education: A review. IEEE 
Access, 8, 75264–75278. 
Deho, O. B., Joksimovic, S., Li, J., Zhan, C., Liu, J., & Liu, L. (2022). Should learning 
analytics models include sensitive attributes? Explaining the why. IEEE Transactions 
on Learning Technologies, 1–13. https://doi.org/10.1109/TLT.2022.3226474 
Fang, B. L., Jiang, M., Cheng, P. Y., Shen, J., & Fang, Y. (2020). Achieving outcome 
fairness in machine learning models for social decision problems. In C. Bessiere 
(Ed.), Proceedings of the twentiy-ninth ioint conference in artificial intelligene (issue 29th 
international joint conference on artificial intelligence (pp. 444–450). 
Garrett, N., Beard, N., & Fiesler, C. (2020). More than" if Time Allows" the role of ethics 
in AI education. In AAAI/ACM conference on AI. Ethics, and Society.  
Gorur, R., Hoon, L., & Kowal, E. (2020). Computer science ethics education in Australia - 
a work in progress. In 2020 IEEE international conference on teaching, assessment, and 
learning for engineering, TALE 2020 (pp. 945–947). https://doi.org/10.1109/ 
TALE48869.2020.9368375 
Holmes, W., Bialik, M., & Fadel, C. (2023). Artificial intelligence in education. Globethics 
Publications. https://doi.org/10.58863/20.500.12424/4273108 
Holmes, W., Porayska-Pomsta, K., Holstein, K., Sutherland, E., Baker, T., Shum, S. B., 
Santos, O. C., Rodrigo, M. T., Cukurova, M., Bittencourt, I. I., & Koedinger, K. R. 
(2021). Ethics of AI in education: Towards a community-wide framework. 
International Journal of Artificial Intelligence in Education, 32(3), 504–526. https://doi. 
org/10.1007/s40593-021-00239-1 
Inuwa-Dutse, I. (2023). FATE in AI: Towards algorithmic inclusivity and accessibility. 
https://doi.org/10.48550/ar Xiv.2301.01590. ar Xiv preprint. 
Islam, S. R., Russell, I., Eberle, W., & Dicheva, D. (2022). Incorporating the concepts of 
fairness and bias into an undergraduate computer science course to promote fair 
automated decision systems. In 53rd annual ACM technical symposium on computer 
B. Memarian and T. Doleck                                                                                                                                                                                                                  

--- Page 12 ---

Computers and Education: Artificial Intelligence 5 (2023) 100152

science education, SIGCSE 2022 (p. 1075). https://doi.org/10.1145/ 
3478432.3499043 
Javed, R. T., Nasir, O., Borit, M., Vanh´ee, L., Zea, E., Gupta, S., Vinuesa, R., & Qadir, J. 
(2022). Get out of the BAG! Silos in AI ethics education: Unsupervised topic 
modeling analysis of global AI curricula.  Journal of Artificial Intelligence Research, 73, 
933–965. https://doi.org/10.1613/jair.1.13550 
Jiang, W., & Pardos, Z. A. (2021). Towards equity and algorithmic fairness in student 
grade prediction. In 4th AAAI/ACM conference on artificial intelligence, ethics, and 
society, AIES 2021 (pp. 608–617). https://doi.org/10.1145/3461702.3462623 
Jobin, A., Ienca, M., & Vayena, E. (2019). The global landscape of AI ethics guidelines. 
Nature Machine Intelligence, 1(9), 389–399. 
Khosravi, H., Shum, S. B., Chen, G., Conati, C., Tsai, Y. S., Kay, J., … Gaˇsevi´c, D. (2022). 
Explainable artificial intelligence in education. Computers in Education: Artificial 
Intelligence, 3, Article 100074. 
Kim, J. Y., & Cho, S. B. (2022). An information theoretic approach to reducing 
algorithmic bias for machine learning. Neurocomputing, 500, 26–38. https://doi.org/ 
10.1016/j.neucom.2021.09.081 
Kobis, L., & Mehner, C. (2021). Ethical questions raised by AI-supported mentoring in 
higher education. Frontiers in Artificial Intelligence, 4. https://doi.org/10.3389/ 
frai.2021.624050 
Kong, S.-C., Cheung, W. M.-Y., & Zhang, G. (2023). Evaluating an artificial intelligence 
literacy programme for developing university students’ conceptual understanding, 
literacy, empowerment and ethical awareness. Educational Technology & Society, 26 
(1), 16–30. https://doi.org/10.30191/ETS.202301_26(1).0002 
Litman, D., Zhang, H. R., Correnti, R., Matsumura, L. C., & Wang, E. (2021). A fairness 
evaluation of automated methods for scoring text evidence usage in writing. 
Artificial Intelligence in Education: 22nd International Conference, AIED 2021, 
Utrecht, The Netherlands. In I. Roll, D. Mc Namara, S. Sosnovsky, R. Luckin, & 
V. Dimitrova (Eds.). Issues 22nd international conference on artificial intelligence in 
education (AIED)-Mind the gap-AIED for equity and inclusion (Vol. 12748, pp. 
255–267). https://doi.org/10.1007/978-3-030-78292-4_21. 
KPMG. (2019). Artificial intelligence perspectives on FATE in AI. Retrieved from https:// 
assets.kpmg.com/content/dam/kpmg/au/pdf/2019/artificial-intelligence-perspect 
ives-on-fate-in-ai.pdf. 
Li, C., Xing, W., & Leite, W. (2022). Using fair AI to predict students’ math learning 
outcomes in an online platform. Interactive Learning Environments. https://doi.org/ 
10.1080/10494820.2022.2115076 
Lucic, A., Bleeker, M., Jullien, S., Bhargav, S., & de Rijke, M. (2022). Reproducibility as a 
mechanism for teaching fairness, accountability, confidentiality, and transparency in 
artificial intelligence. In , Vol. 36. 36th AAAI conference on artificial intelligence, AAAI 
2022 (pp. 12792–12800). 
Marcinkowski, F., Kieslich, K., Starke, C., & Lünich, M. (2020). Implications of AI (un-) 
fairness in higher education admissions: The effects of perceived AI (un-) fairness on 
exit, voice and organizational reputation. In 2020 conference on fairness, 
accountability, and transparency (pp. 122–130). https://doi.org/10.1145/ 
3351095.3372867 
Mashhadi, A., Zolyomi, A., & Quedado, J. (2022). A case study of integrating fairness 
visualization tools in machine learning education. 2022 CHI Conference on human Factors 
in computing systems, CHI EA 2022. https://doi.org/10.1145/3491101.3503568 
Merriam-Webster. (2023). Merriam-webster dictionary. https://www.merriam-webster. 
com/dictionary. 
Microsoft. (2023). FATE: Fairness, accountability, transparency, and ethics in AI. htt 
ps://www.microsoft.com/en-us/research/theme/fate/. 
Nwana, H. S. (1990). Intelligent tutoring systems: An overview. Artificial Intelligence 
Review, 4(4), 251–277. 
Pagallo, U. (2017). From automation to autonomous systems: A legal phenomenology 
with problems of accountability. In C. Sierra (Ed.), Proceedings of the twenty-sixth joint 
Conference in artificial intelligence (issue 26th international joint conference on artificial 
intelligence (IJCAI) (pp. 17–23). 
Pereira, F. D., Fonseca, S. C., Oliveira, E. H. T., Cristea, A. I., Bellhauser, H., 
Rodrigues, L., Oliveira, D. B. F., Isotani, S., & Carvalho, L. S. G. (2021). Explaining 
individual and collective programming students’ behavior by interpreting a black- 
box predictive model. IEEE Access, 9, 117097–117119. https://doi.org/10.1109/ 
ACCESS.2021.3105956 
Prinsloo, P., & Slade, S. (2017). An elephant in the learning analytics room - the 
obligation to act. In 7th international conference on learning analytics and knowledge, 
LAK 2017 (pp. 46–55). https://doi.org/10.1145/3027385.3027406 
Quan, T. K., Zhu, F., Liu, Q., & Li, F. Z. (2023). Learning fair representations for accuracy 
parity (Vol. 119). Engineering Applicaitons of Artificial Intelligence. https://doi.org/ 
10.1016/j.engappai.2023.105819 
Raji, I. D., Scheuerman, M. K., & Amironesei, R. (2021). You can’t sit with us: 
Exclusionary pedagogy in AI ethics education. In 2021 ACM conference on fairness, 
accountability, and transparency (pp. 515–525). 
Regulation, G. D. P. (2018). General data protection regulation (GDPR). Intersoft 
Consulting, 24(1). 
Roll, I., & Wylie, R. (2016). Evolution and revolution in artificial intelligence in 
education. International Journal of Artificial Intelligence in Education, 26, 582–599. 
https://doi.org/10.1007/s40593-016-0110-3 
Segal-Halevi, E., Hassidim, A., & Aziz, H. (2020). Fair allocation with diminishing 
differences. Journal of Artificial Intelligence Research, 67, 471–507. https://doi.org/ 
10.24963/ijcai.2017/174 
Shin, D., Rasul, A., & Fotiadis, A. (2022). Why am I seeing this? Deconstructing algorithm 
literacy through the lens of users. Internet Research, 32(4), 1214–1234. https://doi. 
org/10.1108/INTR-02-2021-0087 
Shulner-Tal, A., Kuflik, T., & Kliger, D. (2023). Enhancing fairness perception - towards 
human-centred AI and personalized explanations understanding the factors 
influencing laypeople’s fairness perceptions of algorithmic decisions. International 
Journal of Human-Computer Interaction, 39(7), 1455–1482. https://doi.org/10.1080/ 
10447318.2022.2095705 
Sikdar, S., Lemmerich, F., & Strohmaier, M. (2022). Get Fair: Generalized fairness tuning 
of classification models. 022 ACM Conference on Fairness, Accountability, and 
Transparency, 289–299. 
Sosnovsky, S., & Brusilovsky, P. (2015). Evaluation of topic-based adaptation and student 
modeling in Quiz Guide. User Modeling and User-Adapted Interaction, 25(4), 371–424. 
https://doi.org/10.1007/s11257-015-9164-4 
Tang, K. Y., Chang, C. Y., & Hwang, G. J. (2021). Trends in artificial intelligence- 
supported e-learning: A systematic review and co-citation network analysis 
(1998–2019). Interactive Learning Environments, 1–19. 
Ten´orio, K., Olari, V., Chikobava, M., & Romeike, R. (2023). Artificial intelligence 
literacy research field: A bibliometric analysis from 1989 to 2021. In 54th ACM 
technical symposium on computer science education, SIGCSE 2023 (Vol. 1, pp. 
1083–1089). https://doi.org/10.1145/3545945.3569874 
Ungerer, L., & Slade, S. (2022). Ethical considerations of artificial intelligence in learning 
analytics in distance education contexts. In Springer Briefs in open and distance 
education (pp. 105–120). Springer Science and Business Media. https://doi.org/ 
10.1007/978-981-19-0786-9_8.  
Williams, T., Zhu, Q., & Grollman, D. (2020). An experimental ethics approach to robot 
ethics education. In 34th AAAI conference on artificial intelligence (pp. 13485–13492). 
https://doi.org/10.1609/aaai.v34i09.7067 
Woolf, B. (1991). AI in education. In Encyclopedia of artificial intelligence (2nd ed.). NY: 
John Wiley & Sons.  
Woolf, B. P. (2015). AI and education: Celebrating 30 Years of marriage. AIED 
Workshops, 4, 38–47. 
Woolf, B. (2022). Introduction to IJAIED special issue, FATE in AIED. International 
Journal of Artificial Intelligence in Education, 32(3), 501–503. 
Woolf, B. P., Lane, H. C., Chaudhri, V. K., & Kolodner, J. L. (2013). AI grand challenges 
for education. AI Magazine, 34(4), 66–84. 
Zawacki-Richter, O., Marin, V. I., Bond, M., & Gouverneur, F. (2019). Systematic review 
of research on artificial intelligence applications in higher education - where are the 
educators? International Journal of Education Technology in Higher Education, 16(1 PG- 
). https://doi.org/10.1186/s41239-019-0171-0 
Zembylas, M. (2023). A decolonial approach to AI in higher education teaching and 
learning: Strategies for undoing the ethics of digital neocolonialism. Learning, Media 
and Technology, 48(1), 25–37. https://doi.org/10.1080/17439884.2021.2010094 
B. Memarian and T. Doleck
