#!/usr/bin/env python3
"""
Metadata Revision Script
This script revises the paperMetaData.md file by combining it with the full paper content
to create a more comprehensive and accurate metadata file.

Author: Dr Simon Wang
Date: October 2024
"""

import os
import re
from pathlib import Path
from datetime import datetime

class MetadataReviser:
    def __init__(self, metadata_path, full_paper_path):
        self.metadata_path = Path(metadata_path)
        self.full_paper_path = Path(full_paper_path)
    
    def read_file(self, file_path):
        """Read file content safely."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except FileNotFoundError:
            print(f"❌ File not found: {file_path}")
            return None
        except Exception as e:
            print(f"❌ Error reading {file_path}: {e}")
            return None
    
    def extract_headings_from_full_paper(self, content):
        """Extract all headings from the full paper content."""
        if not content:
            return []
        
        headings = []
        lines = content.split('\n')
        
        for line in lines:
            line = line.strip()
            if line.startswith('#'):
                # Extract heading level and text
                level = len(line) - len(line.lstrip('#'))
                text = line.lstrip('# ').strip()
                if text:
                    headings.append({'level': level, 'text': text, 'markdown': line})
        
        return headings
    
    def extract_abstract(self, content):
        """Extract abstract from the full paper."""
        if not content:
            return None
        
        lines = content.split('\n')
        abstract_started = False
        abstract_lines = []
        
        for line in lines:
            line = line.strip()
            
            # Look for abstract section
            if re.match(r'^#+\s*abstract\s*$', line, re.IGNORECASE):
                abstract_started = True
                continue
            
            # Stop at next heading
            if abstract_started and line.startswith('#'):
                break
            
            # Collect abstract content
            if abstract_started and line:
                abstract_lines.append(line)
        
        return '\n'.join(abstract_lines) if abstract_lines else None
    
    def extract_bibliographic_info(self, content):
        """Extract bibliographic information from the content."""
        bib_info = {}
        
        if not content:
            return bib_info
        
        # Look for common patterns
        lines = content.split('\n')
        
        for i, line in enumerate(lines):
            line = line.strip()
            
            # Look for title (usually one of the first non-metadata lines)
            if not bib_info.get('title') and len(line) > 10 and not line.startswith('#'):
                # Skip page markers and common metadata lines
                if not any(skip in line.lower() for skip in ['page', 'converted', 'generated', 'date:']):
                    if re.search(r'[a-zA-Z]', line):  # Contains letters
                        bib_info['title'] = line
            
            # Look for author patterns
            if re.search(r'author|by\s+', line, re.IGNORECASE):
                next_lines = lines[i:i+3] if i < len(lines)-2 else lines[i:]
                for next_line in next_lines:
                    if re.search(r'[A-Z][a-z]+\s+[A-Z][a-z]+', next_line):
                        bib_info['authors'] = next_line.strip()
                        break
            
            # Look for DOI
            if 'doi' in line.lower() or re.search(r'10\.\d+/', line):
                bib_info['doi'] = line.strip()
        
        return bib_info
    
    def create_revised_metadata(self, original_metadata, full_paper_content):
        """Create revised metadata combining original and extracted information."""
        
        # Extract information from full paper
        headings = self.extract_headings_from_full_paper(full_paper_content)
        abstract = self.extract_abstract(full_paper_content)
        bib_info = self.extract_bibliographic_info(full_paper_content)
        
        # Build revised metadata
        revised_content = []
        
        # Header
        revised_content.append("# Enhanced Paper Metadata and Structure")
        revised_content.append("")
        revised_content.append(f"*Last updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*")
        revised_content.append(f"*Generated by Metadata Revision Script*")
        revised_content.append("")
        
        # Bibliographic Information
        revised_content.append("## Bibliographic Information")
        revised_content.append("")
        
        # Extract title from original or use detected
        if "**Title:**" in original_metadata:
            title_match = re.search(r'\*\*Title:\*\*\s*(.+)', original_metadata)
            if title_match:
                revised_content.append(f"**Title:** {title_match.group(1)}")
        elif bib_info.get('title'):
            revised_content.append(f"**Title:** {bib_info['title']}")
        
        # Extract authors
        if "**Authors:**" in original_metadata:
            authors_match = re.search(r'\*\*Authors:\*\*\s*(.+)', original_metadata)
            if authors_match:
                revised_content.append(f"**Authors:** {authors_match.group(1)}")
        elif bib_info.get('authors'):
            revised_content.append(f"**Authors:** {bib_info['authors']}")
        
        # Extract journal/publication
        if "**Journal/Publication:**" in original_metadata:
            journal_match = re.search(r'\*\*Journal/Publication:\*\*\s*(.+)', original_metadata)
            if journal_match:
                revised_content.append(f"**Journal/Publication:** {journal_match.group(1)}")
        
        # Extract DOI
        if "**DOI:**" in original_metadata:
            doi_match = re.search(r'\*\*DOI:\*\*\s*(.+)', original_metadata)
            if doi_match:
                revised_content.append(f"**DOI:** {doi_match.group(1)}")
        elif bib_info.get('doi'):
            revised_content.append(f"**DOI:** {bib_info['doi']}")
        
        revised_content.append("")
        
        # Abstract
        revised_content.append("## Abstract")
        revised_content.append("")
        if abstract:
            revised_content.append(abstract)
        elif "## Abstract" in original_metadata:
            # Extract original abstract
            abstract_section = re.search(r'## Abstract\n\n(.+?)\n\n##', original_metadata, re.DOTALL)
            if abstract_section:
                revised_content.append(abstract_section.group(1).strip())
        revised_content.append("")
        
        # Complete Document Structure
        revised_content.append("## Complete Document Structure")
        revised_content.append("")
        revised_content.append("*This section provides a comprehensive outline of the paper's structure.*")
        revised_content.append("")
        
        if headings:
            current_level = 0
            for heading in headings:
                level = heading['level']
                text = heading['text']
                
                # Skip very generic headings and page markers
                if any(skip in text.lower() for skip in ['page', 'converted', 'generated']):
                    continue
                
                # Create hierarchical structure
                indent = "  " * (level - 1) if level > 1 else ""
                if level == 1:
                    revised_content.append(f"### {text}")
                elif level == 2:
                    revised_content.append(f"#### {text}")
                elif level >= 3:
                    revised_content.append(f"{indent}- {text}")
                
                current_level = level
        else:
            revised_content.append("*Structure extraction in progress - please refer to the full paper document.*")
        
        revised_content.append("")
        
        # Missing Sections Analysis
        revised_content.append("## Content Analysis")
        revised_content.append("")
        revised_content.append("### Key Sections Identified")
        revised_content.append("")
        
        # Analyze what sections were found
        section_analysis = self.analyze_sections(headings)
        for analysis in section_analysis:
            revised_content.append(f"- {analysis}")
        
        revised_content.append("")
        revised_content.append("### Extraction Notes")
        revised_content.append("")
        revised_content.append("- This metadata was enhanced using automated PDF extraction")
        revised_content.append("- Some formatting and sub-sections may require manual verification")
        revised_content.append("- Sub-items marked with letters (a), b), etc.) have been identified where possible")
        revised_content.append("")
        
        # Footer
        revised_content.append("---")
        revised_content.append("")
        revised_content.append("*This enhanced metadata combines the original extraction with full paper analysis.*")
        revised_content.append(f"*Full paper content available in: paperFull.md*")
        
        return '\n'.join(revised_content)
    
    def analyze_sections(self, headings):
        """Analyze the extracted headings to provide insights."""
        analysis = []
        
        if not headings:
            return ["No clear document structure detected - may require manual review"]
        
        # Count different levels
        level_counts = {}
        for heading in headings:
            level = heading['level']
            level_counts[level] = level_counts.get(level, 0) + 1
        
        analysis.append(f"Document contains {len(headings)} total headings")
        
        for level, count in sorted(level_counts.items()):
            level_name = {1: "Main titles", 2: "Major sections", 3: "Subsections", 4: "Sub-subsections"}.get(level, f"Level {level}")
            analysis.append(f"{level_name}: {count} found")
        
        # Check for common academic paper sections
        heading_texts = [h['text'].lower() for h in headings]
        common_sections = ['abstract', 'introduction', 'methodology', 'results', 'discussion', 'conclusion', 'references']
        
        found_sections = [section for section in common_sections if any(section in text for text in heading_texts)]
        if found_sections:
            analysis.append(f"Standard academic sections found: {', '.join(found_sections)}")
        
        return analysis
    
    def revise(self):
        """Main revision method."""
        print("📝 Revising paperMetaData.md...")
        
        # Read original metadata
        original_metadata = self.read_file(self.metadata_path)
        if not original_metadata:
            print("❌ Could not read original metadata file")
            return False
        
        # Read full paper content
        full_paper_content = self.read_file(self.full_paper_path)
        if not full_paper_content:
            print("⚠️  Could not read full paper file - proceeding with original metadata only")
            full_paper_content = ""
        
        # Create revised metadata
        revised_metadata = self.create_revised_metadata(original_metadata, full_paper_content)
        
        # Create backup of original
        backup_path = self.metadata_path.with_suffix('.md.backup')
        try:
            with open(backup_path, 'w', encoding='utf-8') as f:
                f.write(original_metadata)
            print(f"📋 Backup created: {backup_path}")
        except Exception as e:
            print(f"⚠️  Could not create backup: {e}")
        
        # Write revised metadata
        try:
            with open(self.metadata_path, 'w', encoding='utf-8') as f:
                f.write(revised_metadata)
            print(f"✅ Revised metadata saved to: {self.metadata_path}")
            return True
        except Exception as e:
            print(f"❌ Error writing revised metadata: {e}")
            return False

def main():
    """Main function."""
    # Define paths
    metadata_path = "/Users/simonwang/Library/CloudStorage/OneDrive-HongKongBaptistUniversity/OneDriveCursor/vibeCoding101/vibeCoding101/PolyUGuestLecture10Oct/output/paperMetaData.md"
    full_paper_path = "/Users/simonwang/Library/CloudStorage/OneDrive-HongKongBaptistUniversity/OneDriveCursor/vibeCoding101/vibeCoding101/PolyUGuestLecture10Oct/output/paperFull.md"
    
    # Create reviser and run
    reviser = MetadataReviser(metadata_path, full_paper_path)
    success = reviser.revise()
    
    if success:
        print("🎉 Metadata revision completed successfully!")
        print(f"📄 Original: {metadata_path}.backup")
        print(f"📝 Revised: {metadata_path}")
    else:
        print("❌ Metadata revision failed")
    
    return success

if __name__ == "__main__":
    main()